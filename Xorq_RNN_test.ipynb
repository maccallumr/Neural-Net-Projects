{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTUfRIkkE197",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dc4b8b6-754b-479e-e74e-af882d79233f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.2`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.2\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "path_to_file = 'Xorq.txt'\n",
        "\n",
        "#tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "#Use your own file \n",
        "#path_to_file = list(\"name\".upload().keys())[0]\n",
        "\n",
        "# Read, then decode for py2 compat.\n",
        "text =  open(path_to_file, 'rb').read().decode(encoding='utf-8', errors='ignore')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYV69olsMQEG"
      },
      "outputs": [],
      "source": [
        "\n",
        "#######Preprocessing/Encoding########\n",
        "\n",
        "#encode each character in the text with an integer \n",
        "vocab = sorted(set(text))\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)} #give us an integer for every letter in vocab\n",
        "idx2char = np.array(vocab) #turns initial vocabulary to an array to use index at which letter appears as a reverse mapping\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)\n",
        "\n",
        "# lets look at how part of our text is encoded\n",
        "#print(\"Text:\", text[:13])\n",
        "#print(\"Encoded:\", text_to_int(text[:13]))\n",
        "\n",
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()  #in the event we pass different objects into here\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "#print(text_as_int[:30])\n",
        "#print(int_to_text(text_as_int[:30]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xwkm75NrPBVd"
      },
      "outputs": [],
      "source": [
        "#############Create Training Examples############\n",
        "\n",
        "#create a stream of characters\n",
        "seq_length = 100  # length of sequence for a training example\n",
        "examples_per_epoch = len(text)//(seq_length+1) #need 101 characters, bc we going to shift them all by one\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int) #convert entire string dataset into characters (all characters in dataset object)\n",
        "\n",
        "#The training example we will prepare will use a se1_length sequence as input and a seq_length sequence as the output where the sequence is the original sequence shifted by one to the right\n",
        "#input: Hell | output: ello\n",
        "\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True) #drop remainder drops leftover that can't be batched\n",
        "\n",
        "def split_input_target(chunk):  # for the example: hello\n",
        "    input_text = chunk[:-1]  # hell\n",
        "    target_text = chunk[1:]  # ello\n",
        "    return input_text, target_text  # hell, ello\n",
        "\n",
        "dataset = sequences.map(split_input_target)  # we use map to apply the above function to every entry\n",
        "\n",
        "#visualize this\n",
        "#for x, y in dataset.take(2):\n",
        "#  print(\"\\n\\nEXAMPLE\\n\")\n",
        "#  print(\"INPUT\")\n",
        "#  print(int_to_text(x))\n",
        "#  print(\"\\nOUTPUT\")\n",
        "#  print(int_to_text(y))\n",
        "  \n",
        "BATCH_SIZE = 64 #a batch will be 64 entries of 100 character sequences\n",
        "VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\n",
        "EMBEDDING_DIM = 256 #How big we want every vector (representing words) to be \n",
        "RNN_UNITS = 1024\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True) #switch around sequences so they aren't shown in proper order to prevent overfitting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmFoQowdUBwL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bff7b9e4-8651-4e88-cc92-600958f207a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           23552     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 92)            94300     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,364,828\n",
            "Trainable params: 5,364,828\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#############Building the Model###########\n",
        "\n",
        "#define a function that returns a built model. Right now we're passing the model batches of size 64 for training, but later we will save the model and pass it batches of 1 training sequence to make a prediction \n",
        "#Later we will rebuild the model with the same saved parameters, but with a different batch input size \n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]), #We leave this none because when we run this model later we will not know how long each sequence will be, though right now for training we are using 100 char long sequences\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True, #return the intermediate stage at every step, false tells us what the model found at the last time step\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size) #final layer has the amount of nodes= amount of chars in vocab--->Prob dist to enable the last layer as a predictive layer \n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5qkIsBTUQ4n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c82eaf33-a2ba-46fa-cb22-72132be6cd9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 92) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "############Create Loss Function############\n",
        "\n",
        "#create our own loss function. This is because our model will output a (64, sequence_length, 65) shaped tensor that represents the prob dist of each character at each timestep for every sequence in the batch\n",
        "#training model is fed batches of 64 sequences of length 100 characters \n",
        "#when we make predictions we pass it 1 entry with variable length. This is why we developed the build model function, to deal with different input shapes\n",
        "#the final layer has an output of (batch_size, sequence_length, vocab_size)= (64, 100, 65)\n",
        "#when dense layer is last layer, that means every prediction contains 65 numbers, each of which are probabilities of each char appearing respectively \n",
        "\n",
        "#using model before it's trained, with raandom weights and biases\n",
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)  # ask our model for a prediction on our first batch of training data (64 entries)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")  # print out the output shape\n",
        "  \n",
        "# we can see that the predicition is an array of 64 arrays, one for each entry in the batch\n",
        "#print(len(example_batch_predictions))\n",
        "#print(example_batch_predictions)\n",
        "\n",
        "# lets examine one prediction\n",
        "pred = example_batch_predictions[0]\n",
        "#print(len(pred))\n",
        "#print(pred)\n",
        "# notice this is a 2d array of length 100, where each interior array is the prediction for the next character at each time step\n",
        "#i.e. for each of the 100 chars (each time step bc it's one char per timestep) it returns 65 probabilities\n",
        "\n",
        "# and finally well look at a prediction at the first timestep\n",
        "time_pred = pred[0]\n",
        "#print(len(time_pred))\n",
        "#print(time_pred)\n",
        "# and of course its 65 values representing the probabillity of each character occuring next\n",
        "\n",
        "# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probabillity)\n",
        "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
        "#can't just take the max of the distribution, need to sample the dist instead, not always the highest prob\n",
        "\n",
        "# now we can reshape that array and convert all the integers to numbers to see the actual characters\n",
        "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "\n",
        "predicted_chars  # and this is what the model predicted for training sequence 1\n",
        "\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vo6O2kjuUcJr"
      },
      "outputs": [],
      "source": [
        "############Compile the Model############\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLXPcovQU3zE"
      },
      "outputs": [],
      "source": [
        "############Checkpoints##################\n",
        "\n",
        "#Allows us to load model from a checkpoint and continue trianing it\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkch4ZDeUlJE",
        "outputId": "fca9ef84-921b-4bba-ea31-1e295302d992"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "141/141 [==============================] - 1087s 8s/step - loss: 2.7741\n",
            "Epoch 2/3\n",
            "141/141 [==============================] - 1054s 7s/step - loss: 2.1463\n",
            "Epoch 3/3\n",
            "141/141 [==============================] - 1077s 8s/step - loss: 1.8652\n"
          ]
        }
      ],
      "source": [
        "###########Training the Model#############\n",
        "\n",
        "history = model.fit(data, epochs=3, callbacks=[checkpoint_callback])\n",
        "\n",
        "###########Loading the Model##############\n",
        "\n",
        "#Need to rebuild the model with a new batch size of 1\n",
        "#Notice we are using our function that builds a model \n",
        "\n",
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)\n",
        "\n",
        "#Most recent checkpoint\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "#Load an intermediate checkpoint\n",
        "#checkpoint_num = 10\n",
        "#model.load_weights(tf.train.load_checkpoint(\"./training_checkpoints/ckpt_\" + str(checkpoint_num)))\n",
        "#model.build(tf.TensorShape([1, None]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "h8O7AZHPs_AA"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcvsXP+WHMKxdB2Re2Ap1v"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}